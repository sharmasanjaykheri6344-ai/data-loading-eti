{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q.1 . Data Understanding\n",
        "\n",
        "Identify all data quality issues present in the dataset that can cause problems during data loading.\n",
        "\n",
        "..>> Data Understanding â€“ Data Quality Issues That Can Affect Data Loading\n",
        "\n",
        "When analyzing a dataset before loading it into a database, data warehouse, or BI tool (like Power BI), the following data quality issues commonly cause problems:\n",
        "\n",
        "1. Missing or Null Values\n",
        "\n",
        "Blank or NULL values in mandatory fields (e.g., Customer_ID, Date, Amount).\n",
        "\n",
        "Can break relationships, aggregations, or cause incorrect calculations.\n",
        "\n",
        "2. Duplicate Records\n",
        "\n",
        "Same records appearing more than once due to system errors or repeated data entry.\n",
        "\n",
        "Leads to inflated counts, totals, and misleading analysis.\n",
        "\n",
        "3. Inconsistent Data Formats\n",
        "\n",
        "Dates stored in multiple formats (e.g., DD-MM-YYYY, MM/DD/YYYY).\n",
        "\n",
        "Numbers stored as text.\n",
        "\n",
        "Causes type conversion errors during loading.\n",
        "\n",
        "4. Invalid Data Values\n",
        "\n",
        "Values outside expected ranges (e.g., negative sales amounts, age = 200).\n",
        "\n",
        "Incorrect categorical values (e.g., â€œMâ€, â€œMaleâ€, â€œmaleâ€).\n",
        "\n",
        "Results in inaccurate analysis and failed validations.\n",
        "\n",
        "5. Data Type Mismatches\n",
        "\n",
        "Numeric fields containing characters (â‚¹, commas, text).\n",
        "\n",
        "Date columns containing text values.\n",
        "\n",
        "Can cause load failures or forced truncation.\n",
        "\n",
        "6. Referential Integrity Issues\n",
        "\n",
        "Foreign key values in one table not matching primary keys in another.\n",
        "\n",
        "Example: Customer_ID in Sales table not found in Customers table.\n",
        "\n",
        "Leads to broken relationships in the data model.\n",
        "\n",
        "7. Inconsistent Naming Conventions\n",
        "\n",
        "Different spellings or casing for the same entity (e.g., â€œMumbaiâ€, â€œmumbaiâ€, â€œMUMâ€).\n",
        "\n",
        "Makes grouping and filtering unreliable.\n",
        "\n",
        "8. Extra Spaces and Special Characters\n",
        "\n",
        "Leading/trailing spaces in text fields.\n",
        "\n",
        "Hidden characters or symbols.\n",
        "\n",
        "Causes mismatches during joins and filters.\n",
        "\n",
        "9. Outdated or Irrelevant Data\n",
        "\n",
        "Old records that should be archived.\n",
        "\n",
        "Fields no longer used but still present.\n",
        "\n",
        "Increases load time and storage unnecessarily.\n",
        "\n",
        "10. Incorrect or Missing Headers\n",
        "\n",
        "Column names missing, duplicated, or unclear.\n",
        "\n",
        "Makes mapping fields during ETL difficult.\n",
        "\n",
        "11. Encoding Issues\n",
        "\n",
        "Special characters not displayed correctly due to encoding mismatches (UTF-8 vs ASCII).\n",
        "\n",
        "Can corrupt text fields during loading.\n",
        "\n",
        "Why Identifying These Issues Matters\n",
        "\n",
        "Prevents ETL failures\n",
        "\n",
        "Ensures accurate reporting\n",
        "\n",
        "Improves performance\n",
        "\n",
        "Maintains data integrity.\n",
        "\n",
        "Q.2 . Primary Key Validation\n",
        "\n",
        "Assume is the Primary Key.\n",
        "\n",
        "a) Is the dataset violating the Primary Key rule?\n",
        "b) Which record(s) cause this violation?\n",
        "\n",
        "\n",
        "..>> Primary Key Validation\n",
        "\n",
        "Assumption: Customer_ID is the Primary Key of the dataset.\n",
        "\n",
        "a) Is the dataset violating the Primary Key rule?\n",
        "\n",
        "Yes, the dataset violates the Primary Key rule if any of the following are present in the Customer_ID column:\n",
        "\n",
        "Duplicate values\n",
        "\n",
        "NULL / blank values\n",
        "\n",
        "A valid primary key must be unique and NOT NULL for every record.\n",
        "\n",
        "b) Which record(s) cause this violation?\n",
        "\n",
        "The violation is caused by records where Customer_ID is:\n",
        "\n",
        "Repeated (Duplicates)\n",
        "\n",
        "Example:\n",
        "\n",
        "Customer_ID\n",
        "C101\n",
        "C102\n",
        "C101   â† duplicate\n",
        "\n",
        "\n",
        "Here, C101 violates the uniqueness rule.\n",
        "\n",
        "Missing / NULL\n",
        "\n",
        "Example:\n",
        "\n",
        "Customer_ID\n",
        "NULL\n",
        "\n",
        "\n",
        "Any NULL value violates the NOT NULL rule.\n",
        "\n",
        "How to Identify Violating Records (Conceptually)\n",
        "\n",
        "Duplicate check:\n",
        "Group by Customer_ID and find values with count > 1\n",
        "\n",
        "Null check:\n",
        "Filter records where Customer_ID IS NULL\n",
        "\n",
        "Conclusion\n",
        "\n",
        "âŒ Primary Key rule is violated\n",
        "\n",
        "ðŸš« Violating records include:\n",
        "\n",
        "Rows with duplicate Customer_ID values\n",
        "\n",
        "Rows with NULL Customer_ID\n",
        "\n",
        "Q.2 Which column(s) contain missing values?\n",
        "\n",
        "a) List the affected records\n",
        "b) Explain why loading these records without handling missing values is risky?\n",
        "\n",
        "..>> Iâ€™m missing one key thing: the actual dataset (or at least the table structure + sample rows).\n",
        "Without seeing the data, I canâ€™t truthfully name which specific column(s) contain missing values or list the exact affected records.\n",
        "\n",
        "That said, hereâ€™s how this is typically answered in exams / ETL reviews, and how youâ€™d explain the risk part clearly ðŸ‘‡\n",
        "\n",
        "a) How to identify columns with missing values (and list affected records)\n",
        "Step 1: Identify columns with missing values\n",
        "\n",
        "A column is considered to have missing values if it contains NULL, blank, or empty values.\n",
        "\n",
        "Example (conceptual):\n",
        "\n",
        "Customer_ID\tName\tAge\tIncome\n",
        "C101\tRahul\t30\t50000\n",
        "C102\tAnjali\tNULL\t60000\n",
        "C103\tSuresh\t28\tNULL\n",
        "\n",
        "ðŸ‘‰ Columns with missing values:\n",
        "\n",
        "Age\n",
        "\n",
        "Income\n",
        "\n",
        "Step 2: List affected records\n",
        "\n",
        "Affected records are rows where one or more columns have missing values.\n",
        "\n",
        "Example affected records:\n",
        "\n",
        "Customer_ID\tMissing Column\n",
        "C102\tAge\n",
        "C103\tIncome\n",
        "Typical SQL to detect missing values\n",
        "SELECT *\n",
        "FROM Customers\n",
        "WHERE Age IS NULL\n",
        "   OR Income IS NULL;\n",
        "\n",
        "b) Why loading records with missing values is risky\n",
        "\n",
        "Loading data without handling missing values can cause serious business and technical issues:\n",
        "\n",
        "1. Incorrect analysis & reporting\n",
        "\n",
        "Aggregations like AVG, SUM, COUNT may give misleading results\n",
        "\n",
        "Example: Average income will be lower or inaccurate if NULLs are ignored\n",
        "\n",
        "2. Broken business rules\n",
        "\n",
        "Mandatory fields (Age, Customer_ID, Amount) being NULL can violate data quality rules\n",
        "\n",
        "This may cause ETL job failures or rejected records downstream\n",
        "\n",
        "3. Errors in joins and relationships\n",
        "\n",
        "Missing foreign keys can lead to orphan records\n",
        "\n",
        "Reports may show incomplete or inconsistent data\n",
        "\n",
        "4. Poor decision-making\n",
        "\n",
        "Business users may make decisions based on incomplete or misleading insights\n",
        "\n",
        "Example: Incorrect customer segmentation due to missing demographic data\n",
        "\n",
        "5. Issues in ML / advanced analytics\n",
        "\n",
        "Many models cannot handle NULL values\n",
        "\n",
        "Missing data can bias predictions or cause model failures\n",
        "\n",
        "Q.5 Identify records where Sales_Amount violates expected data type rules.\n",
        "\n",
        "a) Which record(s) will fail numeric validation?\n",
        "b) What would happen if this dataset is loaded data sales  into a SQL table with as DECIMAL?\n",
        "\n",
        "..>>  a) Record(s) that will fail numeric validation\n",
        "\n",
        "Sales_Amount violates data type rules if it contains:\n",
        "\n",
        "Alphabetic characters\n",
        "\n",
        "Alphanumeric values\n",
        "\n",
        "Special symbols (â‚¹, $, commas if not handled)\n",
        "\n",
        "Empty strings or text like \"N/A\", \"NA\", \"NULL\"\n",
        "\n",
        "Example dataset\n",
        "Order_ID\tSales_Amount\n",
        "O101\t15000\n",
        "O102\t12,000\n",
        "O103\tN/A\n",
        "O104\tâ‚¹5000\n",
        "O105\t8500.75\n",
        "O106\t(blank)\n",
        "âŒ Records failing numeric validation\n",
        "Order_ID\tReason\n",
        "O102\tContains comma\n",
        "O103\tNon-numeric text\n",
        "O104\tCurrency symbol\n",
        "O106\tBlank / empty\n",
        "\n",
        "âœ” Valid records: O101, O105\n",
        "\n",
        "b) What happens if loaded into a SQL table as DECIMAL\n",
        "Table definition\n",
        "Sales_Amount DECIMAL(10,2)\n",
        "\n",
        "Load behavior\n",
        "âŒ Invalid values\n",
        "\n",
        "12,000\n",
        "\n",
        "N/A\n",
        "\n",
        "â‚¹5000\n",
        "\n",
        "blank values\n",
        "\n",
        "ðŸ‘‰ These will cause:\n",
        "\n",
        "Load failure\n",
        "\n",
        "Conversion errors such as:\n",
        "\n",
        "â€œInvalid numeric valueâ€\n",
        "\n",
        "â€œError converting data type varchar to numericâ€\n",
        "\n",
        "âš  Blank values converted to NULL\n",
        "\n",
        "If ETL converts blanks â†’ NULL\n",
        "\n",
        "Load succeeds, but:\n",
        "\n",
        "Aggregations (SUM, AVG) become inaccurate\n",
        "\n",
        "Records may be silently excluded from calculations\n",
        "\n",
        "Impact on the ETL process\n",
        "\n",
        "Entire batch load may fail or roll back\n",
        "\n",
        "Partial loads create data inconsistency\n",
        "\n",
        "Reporting and financial calculations become unreliable\n",
        "\n",
        "Best practice before loading DECIMAL columns\n",
        "\n",
        "âœ” Remove currency symbols\n",
        "âœ” Remove commas\n",
        "âœ” Convert text placeholders (N/A, NULL) to NULL\n",
        "âœ” Validate numeric format using regex or CAST checks\n",
        "âœ” Add a rejection or error table for invalid rows\n",
        "\n",
        "Final takeaway\n",
        "\n",
        "Records with non-numeric characters in Sales_Amount will fail validation and cause load errors when inserted into a DECIMAL column, making data cleansing mandatory before load.\n",
        "\n",
        "Q.5  Date data sales  Format Consistency\n",
        "\n",
        "..>> This is a classic ETL/data-quality question ðŸ‘\n",
        "Hereâ€™s a clear, structured answer you can use directly.\n",
        "\n",
        "a) Date formats present in the dataset\n",
        "\n",
        "When a single date column has multiple formats, youâ€™ll typically see combinations like:\n",
        "\n",
        "Example values from the same column\n",
        "Date Value\n",
        "2024-01-15\n",
        "15-01-2024\n",
        "01/15/2024\n",
        "15/01/2024\n",
        "Jan 15, 2024\n",
        "20240115\n",
        "Identified date formats\n",
        "Format\tExample\n",
        "YYYY-MM-DD\t2024-01-15\n",
        "DD-MM-YYYY\t15-01-2024\n",
        "MM/DD/YYYY\t01/15/2024\n",
        "DD/MM/YYYY\t15/01/2024\n",
        "Mon DD, YYYY\tJan 15, 2024\n",
        "YYYYMMDD\t20240115\n",
        "b) Why multiple date formats are a problem during data loading\n",
        "1. Conversion failures\n",
        "\n",
        "SQL databases expect one consistent date format\n",
        "\n",
        "Rows with unexpected formats will cause:\n",
        "\n",
        "â€œConversion failed when converting date and/or timeâ€\n",
        "\n",
        "Entire load may fail or partially load\n",
        "\n",
        "2. Ambiguous dates\n",
        "\n",
        "01/02/2024\n",
        "\n",
        "Could mean 1 Feb 2024 or 2 Jan 2024\n",
        "\n",
        "Leads to wrong dates being stored silently\n",
        "\n",
        "3. Sorting and filtering issues\n",
        "\n",
        "Dates stored as text sort incorrectly\n",
        "\n",
        "Time-based filters (monthly, yearly reports) become unreliable\n",
        "\n",
        "4. Broken joins and aggregations\n",
        "\n",
        "Date mismatches break joins with calendar/date dimension tables\n",
        "\n",
        "Aggregations like monthly sales give incorrect results\n",
        "\n",
        "5. BI & reporting errors\n",
        "\n",
        "Tools like Power BI may interpret formats differently based on locale\n",
        "\n",
        "Same date mayMultiple date formats must be standardized before loading because they cause conversion errors, ambiguous interpretations, and incorrect time-based analysi.\n",
        "\n",
        "Q.7. Pre-Load Validation Checklist\n",
        "\n",
        "List the exact pre-load validation checks you would perform on this dataset before loading.\n",
        "\n",
        "..>> Pre-Load Validation Checklist (Before Loading into SQL)\n",
        "1. Schema & Structure Validation\n",
        "\n",
        "âœ” Correct number of columns\n",
        "\n",
        "âœ” Column names match target schema\n",
        "\n",
        "âœ” Column order is correct\n",
        "\n",
        "âœ” No extra or missing columns\n",
        "\n",
        "2. Data Type Validation\n",
        "\n",
        "âœ” Numeric columns contain only numeric values\n",
        "(Sales_Amount, Income, Quantity â†’ no text, symbols, commas)\n",
        "\n",
        "âœ” Date columns contain valid, parsable dates\n",
        "\n",
        "âœ” Text columns do not exceed defined length limits\n",
        "\n",
        "3. Missing Value Checks\n",
        "\n",
        "âœ” Identify NULLs, blanks, empty strings\n",
        "\n",
        "âœ” Verify mandatory columns have no missing values\n",
        "(IDs, transaction dates, amounts)\n",
        "\n",
        "âœ” Decide handling: reject, impute, default, or flag\n",
        "\n",
        "4. Date Format Standardization\n",
        "\n",
        "âœ” Detect multiple date formats\n",
        "\n",
        "âœ” Convert all dates to a single standard format (YYYY-MM-DD)\n",
        "\n",
        "âœ” Validate date ranges (no future or impossible dates)\n",
        "\n",
        "5. Numeric Range & Business Rule Validation\n",
        "\n",
        "âœ” Sales_Amount > 0\n",
        "\n",
        "âœ” Quantity â‰¥ 1\n",
        "\n",
        "âœ” No unrealistic values (negative sales, extreme outliers)\n",
        "\n",
        "âœ” Currency precision matches target (DECIMAL(p,s))\n",
        "\n",
        "6. Referential Integrity Checks\n",
        "\n",
        "âœ” Foreign keys exist in master tables\n",
        "(Customer_ID, Product_ID)\n",
        "\n",
        "âœ” No orphan records\n",
        "\n",
        "7. Duplicate Detection\n",
        "\n",
        "âœ” Check duplicate primary keys\n",
        "\n",
        "âœ” Detect duplicate business keys\n",
        "(same Order_ID + Date)\n",
        "\n",
        "8. Format & Pattern Validation\n",
        "\n",
        "âœ” IDs follow expected pattern (C###, O####)\n",
        "\n",
        "âœ” No special characters in numeric fields\n",
        "\n",
        "âœ” Trim leading/trailing spaces in text fields\n",
        "\n",
        "9. Constraint & Target Table Compatibility\n",
        "\n",
        "âœ” Data fits column size limits\n",
        "\n",
        "âœ” DECIMAL scale/precision not exceeded\n",
        "\n",
        "âœ” NOT NULL constraints satisfied\n",
        "\n",
        "10. Rejection & Logging Readiness\n",
        "\n",
        "âœ” Invalid rows redirected to error/reject table\n",
        "\n",
        "âœ” Error reason captured per record\n",
        "\n",
        "âœ” Row counts matched (source vs accepted vs rejected)\n",
        "\n",
        "Final takeaway\n",
        "\n",
        "Pre-load validation ensures data quality, prevents load failures, and protects downstream reporting and business decisions.\n",
        "\n",
        "Q.8 Q8. Cleaning Strategy\n",
        "\n",
        "Describe the step-by-step cleaning actions required to make this dataset load-ready.\n",
        "\n",
        "..>> Step 1: Initial Data Profiling\n",
        "\n",
        "Scan the dataset to understand:\n",
        "\n",
        "Column data types\n",
        "\n",
        "Missing values\n",
        "\n",
        "Invalid characters\n",
        "\n",
        "Date and numeric inconsistencies\n",
        "\n",
        "Capture row counts and column statistics\n",
        "\n",
        "Step 2: Remove Structural Issues\n",
        "\n",
        "Delete empty rows and columns\n",
        "\n",
        "Fix incorrect headers\n",
        "\n",
        "Ensure consistent column order and naming\n",
        "\n",
        "Step 3: Handle Missing Values\n",
        "\n",
        "Identify NULLs, blanks, and placeholders (NA, N/A)\n",
        "\n",
        "Apply rules:\n",
        "\n",
        "Mandatory fields â†’ reject record\n",
        "\n",
        "Numeric fields â†’ impute / default / flag\n",
        "\n",
        "Text fields â†’ replace with Unknown if allowed\n",
        "\n",
        "Add Missing_Value_Flag columns where required\n",
        "\n",
        "Step 4: Clean and Validate Numeric Columns\n",
        "\n",
        "Remove currency symbols (â‚¹, $)\n",
        "\n",
        "Remove commas and spaces\n",
        "\n",
        "Convert values to numeric\n",
        "\n",
        "Reject non-numeric values\n",
        "\n",
        "Enforce business rules (no negative sales)\n",
        "\n",
        "Step 5: Standardize Date Columns\n",
        "\n",
        "Detect all date formats\n",
        "\n",
        "Convert to a single format (YYYY-MM-DD)\n",
        "\n",
        "Validate logical ranges (no future or invalid dates)\n",
        "\n",
        "Reject ambiguous or invalid dates\n",
        "\n",
        "Step 6: Standardize Categorical Values\n",
        "\n",
        "Normalize inconsistent text values\n",
        "(e.g., â€œmâ€, â€œMaleâ€, â€œMALEâ€ â†’ â€œMaleâ€)\n",
        "\n",
        "Trim leading/trailing spaces\n",
        "\n",
        "Apply consistent casing\n",
        "\n",
        "Step 7: Validate Keys & Relationships\n",
        "\n",
        "Check primary keys for NULLs and duplicates\n",
        "\n",
        "Validate foreign keys against master tables\n",
        "\n",
        "Remove or quarantine orphan records\n",
        "\n",
        "Step 8: Remove Duplicates\n",
        "\n",
        "Identify exact duplicates\n",
        "\n",
        "Identify business duplicates (same Order_ID + Date)\n",
        "\n",
        "Retain the most recent or valid record\n",
        "\n",
        "Step 9: Apply Data Type & Constraint Checks\n",
        "\n",
        "Ensure data fits:\n",
        "\n",
        "VARCHAR length limits\n",
        "\n",
        "DECIMAL precision and scale\n",
        "\n",
        "NOT NULL constraints\n",
        "\n",
        "Re-validate after transformations\n",
        "\n",
        "Step 10: Error Handling & Rejection\n",
        "\n",
        "Move invalid records to a reject/error table\n",
        "\n",
        "Log:\n",
        "\n",
        "Row number\n",
        "\n",
        "Error type\n",
        "\n",
        "Column name\n",
        "\n",
        "Keep original data for audit purposes\n",
        "\n",
        "Step 11: Final Validation Before Load\n",
        "\n",
        "Reconcile row counts:\n",
        "\n",
        "Source = Loaded + Rejected\n",
        "\n",
        "Perform spot checks\n",
        "\n",
        "Validate sample calculations (totals, counts)\n",
        "\n",
        "Final takeaway\n",
        "\n",
        "A structured, step-by-step cleaning strategy ensures the dataset is accurate, consistent, and fully compatible with the target SQL schema before loading.\n",
        "\n",
        "Q.9 . Loading Strategy Selection\n",
        "\n",
        "Assume this dataset represents daily sales data.\n",
        "\n",
        "a) Should a Full Load or Incremental Load be used?\n",
        "b) Justify your choice.\n",
        "\n",
        "..>> This is a textbook ETL design decision question. Hereâ€™s the clear, scoring answer ðŸ‘‡\n",
        "\n",
        "a) Load type to use\n",
        "\n",
        "Incremental Load should be used.\n",
        "\n",
        "b) Justification\n",
        "1. Nature of the data (daily sales)\n",
        "\n",
        "Daily sales data grows continuously\n",
        "\n",
        "New records are added each day\n",
        "\n",
        "Reloading the entire dataset every time is unnecessary\n",
        "\n",
        "2. Performance efficiency\n",
        "\n",
        "Incremental load processes only new or changed records\n",
        "\n",
        "Reduces:\n",
        "\n",
        "Load time\n",
        "\n",
        "CPU and memory usage\n",
        "\n",
        "Network and storage costs\n",
        "\n",
        "3. Data volume scalability\n",
        "\n",
        "Sales data increases rapidly over time\n",
        "\n",
        "Full loads become slower and risk timeouts as data grows\n",
        "\n",
        "Incremental loads scale better for long-term operations\n",
        "\n",
        "4. Reduced risk during loading\n",
        "\n",
        "Smaller data batches mean:\n",
        "\n",
        "Lower chance of job failure\n",
        "\n",
        "Easier troubleshooting and recovery\n",
        "\n",
        "5. Business continuity\n",
        "\n",
        "Enables near real-time or daily reporting\n",
        "\n",
        "Data is available quickly without long reload windows\n",
        "\n",
        "6. Controlled data changes\n",
        "\n",
        "Late-arriving or corrected records can be handled using:\n",
        "\n",
        "Last_Updated_Timestamp\n",
        "\n",
        "Transaction_Date\n",
        "\n",
        "Supports UPSERT (insert + update) logic\n",
        "\n",
        "When a Full Load is still used\n",
        "\n",
        "Initial data load (first time)\n",
        "\n",
        "Major schema or business-rule changes\n",
        "\n",
        "Data corruption recovery\n",
        "\n",
        "Final takeaway\n",
        "\n",
        "For daily sales data, an incremental load is preferred because it is faster, scalable, and minimizes processing and operational risk compared to full loads.\n",
        "\n",
        "Q.10 0. BI Impact Scenario\n",
        "\n",
        "Assume this dataset was loaded without cleaning and connected to a BI dashboard.\n",
        "\n",
        "a) What incorrect results might appear in Total Sales KPI?\n",
        "b) Which records specifically would cause misleading insights?\n",
        "c) Why would BI tools not detect these issues automactically?\n",
        "\n",
        "..>> Great scenario-based question â€” this is exactly how data quality problems surface painfully in BI ðŸ˜…\n",
        "Hereâ€™s a clear, practical answer.\n",
        "\n",
        "a) Incorrect results in the Total Sales KPI\n",
        "\n",
        "If the dataset is loaded without cleaning, the Total Sales KPI can show:\n",
        "\n",
        "1. Underreported total sales\n",
        "\n",
        "Non-numeric values (\"N/A\", \"â‚¹5000\", \"12,000\") are:\n",
        "\n",
        "Ignored\n",
        "\n",
        "Converted to NULL\n",
        "\n",
        "These records are excluded from SUM(Sales_Amount)\n",
        "\n",
        "2. Overreported or distorted totals\n",
        "\n",
        "Negative or incorrect values (-5000, 5000000 typo)\n",
        "\n",
        "Duplicated sales records counted multiple times\n",
        "\n",
        "3. Inconsistent totals across visuals\n",
        "\n",
        "KPI card â‰  table â‰  chart\n",
        "\n",
        "Due to filters silently excluding invalid rows\n",
        "\n",
        "4. Sudden drops or spikes in trends\n",
        "\n",
        "Missing or invalid dates cause sales to fall into wrong periods or disappear\n",
        "\n",
        "b) Records that cause misleading insights\n",
        "Problematic records include:\n",
        "Issue\tExample Record\n",
        "Non-numeric sales\t\"N/A\", \"â‚¹4500\"\n",
        "Sales stored as text\t\"12,000\"\n",
        "Missing sales\tNULL\n",
        "Negative values\t-3000\n",
        "Duplicate transactions\tSame Order_ID twice\n",
        "Invalid dates\t32/01/2024, 01/02/2024 (ambiguous)\n",
        "\n",
        "ðŸ‘‰ These records are ignored, mis-grouped, or double-counted by BI calculations.\n",
        "\n",
        "c) Why BI tools donâ€™t detect these issues automatically\n",
        "1. BI tools assume data is already clean\n",
        "\n",
        "BI tools focus on visualization, not validation\n",
        "\n",
        "They trust the source system\n",
        "\n",
        "2. Silent error handling\n",
        "\n",
        "Invalid numeric values are:\n",
        "\n",
        "Converted to NULL\n",
        "\n",
        "Dropped from calculations without warnings\n",
        "\n",
        "3. Lack of business-rule awareness\n",
        "\n",
        "BI tools donâ€™t know:\n",
        "\n",
        "Sales should never be negative\n",
        "\n",
        "Currency symbols are invalid\n",
        "\n",
        "Which fields are mandatory\n",
        "\n",
        "4. Locale & format assumptions\n",
        "\n",
        "Date and number interpretation depends on:\n",
        "\n",
        "System locale\n",
        "\n",
        "Data source settings\n",
        "\n",
        "Leads to silent misinterpretation\n",
        "\n",
        "5. Aggregations hide row-level issues\n",
        "\n",
        "KPIs show aggregated results\n",
        "\n",
        "Individual bad records are invisible unless explicitly checked\n",
        "\n",
        "Final takeaway\n",
        "\n",
        "BI dashboards can look correct while being fundamentally wrong if upstream data cleaning is skipped, making ETL validation critical.\n",
        "\n"
      ],
      "metadata": {
        "id": "XWbsMOC95uvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJAUJhC35oBX"
      },
      "outputs": [],
      "source": []
    }
  ]
}